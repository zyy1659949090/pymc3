
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>GLM: Linear regression &#8212; PyMC3 3.2 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '3.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GLM: Robust Linear Regression" href="GLM-robust.html" />
    <link rel="prev" title="The Dawid-Skene model with priors" href="dawid-skene.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="GLM:-Linear-regression">
<h1>GLM: Linear regression<a class="headerlink" href="#GLM:-Linear-regression" title="Permalink to this headline">¶</a></h1>
<p>Author: Thomas Wiecki</p>
<p>This tutorial is adapted from a <a class="reference external" href="http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/">blog post by Thomas Wiecki called “The
Inference Button: Bayesian GLMs made easy with
PyMC3”</a>.</p>
<p>This tutorial appeared as a post in a small series on Bayesian GLMs on
my blog:</p>
<ol class="arabic simple">
<li><a class="reference external" href="http://twiecki.github.com/blog/2013/08/12/bayesian-glms-1/">The Inference Button: Bayesian GLMs made easy with
PyMC3</a></li>
<li><a class="reference external" href="http://twiecki.github.io/blog/2013/08/27/bayesian-glms-2/">This world is far from Normal(ly distributed): Robust Regression in
PyMC3</a></li>
<li><a class="reference external" href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">The Best Of Both Worlds: Hierarchical Linear Regression in
PyMC3</a></li>
</ol>
<p>In this blog post I will talk about:</p>
<ul class="simple">
<li>How the Bayesian Revolution in many scientific disciplines is
hindered by poor usability of current Probabilistic Programming
languages.</li>
<li>A gentle introduction to Bayesian linear regression and how it
differs from the frequentist approach.</li>
<li>A preview of <a class="reference external" href="https://github.com/pymc-devs/pymc/tree/pymc3">PyMC3</a>
(currently in alpha) and its new GLM submodule I wrote to allow
creation and estimation of Bayesian GLMs as easy as frequentist GLMs
in R.</li>
</ul>
<p>Ready? Lets get started!</p>
<p>There is a huge paradigm shift underway in many scientific disciplines:
The Bayesian Revolution.</p>
<p>While the theoretical benefits of Bayesian over Frequentist stats have
been discussed at length elsewhere (see <em>Further Reading</em> below), there
is a major obstacle that hinders wider adoption – <em>usability</em> (this is
one of the reasons DARPA wrote out a huge grant to <a class="reference external" href="http://www.darpa.mil/program/probabilistic-programming-for-advancing-machine-Learning">improve
Probabilistic
Programming</a>).</p>
<p>This is mildly ironic because the beauty of Bayesian statistics is their
generality. Frequentist stats have a bazillion different tests for every
different scenario. In Bayesian land you define your model exactly as
you think is appropriate and hit the <em>Inference Button(TM)</em> (i.e.
running the magical MCMC sampling algorithm).</p>
<p>Yet when I ask my colleagues why they use frequentist stats (even though
they would like to use Bayesian stats) the answer is that software
packages like SPSS or R make it very easy to run all those individuals
tests with a single command (and more often then not, they don’t know
the exact model and inference method being used).</p>
<p>While there are great Bayesian software packages like
<a class="reference external" href="http://mcmc-jags.sourceforge.net/">JAGS</a>,
<a class="reference external" href="http://www.mrc-bsu.cam.ac.uk/bugs/">BUGS</a>,
<a class="reference external" href="http://mc-stan.org/">Stan</a> and
<a class="reference external" href="http://pymc-devs.github.io/pymc/">PyMC</a>, they are written for
Bayesians statisticians who know very well what model they want to
build.</p>
<p>Unfortunately, <a class="reference external" href="http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/">“the vast majority of statistical analysis is not
performed by
statisticians”</a>
– so what we really need are tools for <em>scientists</em> and not for
statisticians.</p>
<p>In the interest of putting my code where my mouth is I wrote a submodule
for the upcoming
<a class="reference external" href="https://github.com/pymc-devs/pymc/tree/pymc3">PyMC3</a> that makes
construction of Bayesian Generalized Linear Models (GLMs) as easy as
Frequentist ones in R.</p>
<div class="section" id="Linear-Regression">
<h2>Linear Regression<a class="headerlink" href="#Linear-Regression" title="Permalink to this headline">¶</a></h2>
<p>While future blog posts will explore more complex models, I will start
here with the simplest GLM – linear regression. In general,
frequentists think about Linear Regression as follows:</p>
<div class="math">
\[Y = X\beta + \epsilon\]</div>
<p>where <span class="math">\(Y\)</span> is the output we want to predict (or <em>dependent</em>
variable), <span class="math">\(X\)</span> is our predictor (or <em>independent</em> variable), and
<span class="math">\(\beta\)</span> are the coefficients (or parameters) of the model we want
to estimate. <span class="math">\(\epsilon\)</span> is an error term which is assumed to be
normally distributed.</p>
<p>We can then use Ordinary Least Squares or Maximum Likelihood to find the
best fitting <span class="math">\(\beta\)</span>.</p>
</div>
<div class="section" id="Probabilistic-Reformulation">
<h2>Probabilistic Reformulation<a class="headerlink" href="#Probabilistic-Reformulation" title="Permalink to this headline">¶</a></h2>
<p>Bayesians take a probabilistic view of the world and express this model
in terms of probability distributions. Our above linear regression can
be rewritten to yield:</p>
<div class="math">
\[Y \sim \mathcal{N}(X \beta, \sigma^2)\]</div>
<p>In words, we view <span class="math">\(Y\)</span> as a random variable (or random vector) of
which each element (data point) is distributed according to a Normal
distribution. The mean of this normal distribution is provided by our
linear predictor with variance <span class="math">\(\sigma^2\)</span>.</p>
<p>While this is essentially the same model, there are two critical
advantages of Bayesian estimation:</p>
<ul class="simple">
<li>Priors: We can quantify any prior knowledge we might have by placing
priors on the paramters. For example, if we think that <span class="math">\(\sigma\)</span>
is likely to be small we would choose a prior with more probability
mass on low values.</li>
<li>Quantifying uncertainty: We do not get a single estimate of
<span class="math">\(\beta\)</span> as above but instead a complete posterior distribution
about how likely different values of <span class="math">\(\beta\)</span> are. For example,
with few data points our uncertainty in <span class="math">\(\beta\)</span> will be very
high and we’d be getting very wide posteriors.</li>
</ul>
</div>
<div class="section" id="Bayesian-GLMs-in-PyMC3">
<h2>Bayesian GLMs in PyMC3<a class="headerlink" href="#Bayesian-GLMs-in-PyMC3" title="Permalink to this headline">¶</a></h2>
<p>With the new GLM module in PyMC3 it is very easy to build this and much
more complex models.</p>
<p>First, lets import the required modules.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span>  <span class="o">*</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
<div class="section" id="Generating-data">
<h3>Generating data<a class="headerlink" href="#Generating-data" title="Permalink to this headline">¶</a></h3>
<p>Create some toy data to play around with and scatter-plot it.</p>
<p>Essentially we are creating a regression line defined by intercept and
slope and add data points by sampling from a Normal with the mean set to
the regression line.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">true_intercept</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">true_slope</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="c1"># y = a + b*x</span>
<span class="n">true_regression_line</span> <span class="o">=</span> <span class="n">true_intercept</span> <span class="o">+</span> <span class="n">true_slope</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1"># add noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_regression_line</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Generated data and underlying model&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sampled data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_regression_line</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;true regression line&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-linear_6_0.png" src="../_images/notebooks_GLM-linear_6_0.png" />
</div>
</div>
</div>
<div class="section" id="Estimating-the-model">
<h3>Estimating the model<a class="headerlink" href="#Estimating-the-model" title="Permalink to this headline">¶</a></h3>
<p>Lets fit a Bayesian linear regression model to this data. As you can
see, model specifications in <code class="docutils literal"><span class="pre">PyMC3</span></code> are wrapped in a <code class="docutils literal"><span class="pre">with</span></code>
statement.</p>
<p>Here we use the awesome new <a class="reference external" href="http://arxiv.org/abs/1111.4246">NUTS
sampler</a> (our Inference Button) to
draw 2000 posterior samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># model specifications in PyMC3 are wrapped in a with-statement</span>
    <span class="c1"># Define priors</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">x_coeff</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># Define likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">intercept</span> <span class="o">+</span> <span class="n">x_coeff</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span>
                        <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Inference!</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">3000</span><span class="p">,</span> <span class="n">njobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># draw 3000 posterior samples using NUTS sampling</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 175.98:   6%|▋         | 12609/200000 [00:00&lt;00:11, 15847.31it/s]
Convergence archived at 13800
Interrupted at 13,800 [6%]: Average Loss = 359.45
100%|██████████| 3500/3500 [00:04&lt;00:00, 804.61it/s]
</pre></div></div>
</div>
<p>This should be fairly readable for people who know probabilistic
programming. However, would my non-statistican friend know what all this
does? Moreover, recall that this is an extremely simple model that would
be one line in R. Having multiple, potentially transformed regressors,
interaction terms or link-functions would also make this much more
complex and error prone.</p>
<p>The new <code class="docutils literal"><span class="pre">glm()</span></code> function instead takes a
<a class="reference external" href="http://patsy.readthedocs.org/en/latest/quickstart.html">Patsy</a>
linear model specifier from which it creates a design matrix. <code class="docutils literal"><span class="pre">glm()</span></code>
then adds random variables for each of the coefficients and an
appopriate likelihood to the model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># specify glm and pass in data. The resulting linear model, its likelihood and</span>
    <span class="c1"># and all its parameters are automatically added to our model.</span>
    <span class="n">glm</span><span class="o">.</span><span class="n">GLM</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="s1">&#39;y ~ x&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">3000</span><span class="p">,</span> <span class="n">njobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># draw 3000 posterior samples using NUTS sampling</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 171.74:   7%|▋         | 13463/200000 [00:00&lt;00:12, 14993.68it/s]
Convergence archived at 14100
Interrupted at 14,100 [7%]: Average Loss = 341.38
100%|██████████| 3500/3500 [00:04&lt;00:00, 809.71it/s]
</pre></div></div>
</div>
<p>Much shorter, but this code does the exact same thing as the above model
specification (you can change priors and everything else too if we
wanted). <code class="docutils literal"><span class="pre">glm()</span></code> parses the <code class="docutils literal"><span class="pre">Patsy</span></code> model string, adds random
variables for each regressor (<code class="docutils literal"><span class="pre">Intercept</span></code> and slope <code class="docutils literal"><span class="pre">x</span></code> in this
case), adds a likelihood (by default, a Normal is chosen), and all other
variables (<code class="docutils literal"><span class="pre">sigma</span></code>). Finally, <code class="docutils literal"><span class="pre">glm()</span></code> then initializes the
parameters to a good starting point by estimating a frequentist linear
model using <a class="reference external" href="http://statsmodels.sourceforge.net/devel/">statsmodels</a>.</p>
<p>If you are not familiar with R’s syntax, <code class="docutils literal"><span class="pre">'y</span> <span class="pre">~</span> <span class="pre">x'</span></code> specifies that we
have an output variable <code class="docutils literal"><span class="pre">y</span></code> that we want to estimate as a linear
function of <code class="docutils literal"><span class="pre">x</span></code>.</p>
</div>
<div class="section" id="Analyzing-the-model">
<h3>Analyzing the model<a class="headerlink" href="#Analyzing-the-model" title="Permalink to this headline">¶</a></h3>
<p>Bayesian inference does not give us only one best fitting line (as
maximum likelihood does) but rather a whole posterior distribution of
likely parameters. Lets plot the posterior distribution of our
parameters and the individual samples we drew.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="mi">100</span><span class="p">:])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.figure.Figure at 0x7f4122bcf908&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-linear_15_1.png" src="../_images/notebooks_GLM-linear_15_1.png" />
</div>
</div>
<p>The left side shows our marginal posterior – for each parameter value
on the x-axis we get a probability on the y-axis that tells us how
likely that parameter value is.</p>
<p>There are a couple of things to see here. The first is that our sampling
chains for the individual parameters (left side) seem well converged and
stationary (there are no large drifts or other odd patterns).</p>
<p>Secondly, the maximum posterior estimate of each variable (the peak in
the left side distributions) is very close to the true parameters used
to generate the data (<code class="docutils literal"><span class="pre">x</span></code> is the regression coefficient and <code class="docutils literal"><span class="pre">sigma</span></code>
is the standard deviation of our normal).</p>
<p>In the GLM we thus do not only have one best fitting regression line,
but many. A posterior predictive plot takes multiple samples from the
posterior (intercepts and slopes) and plots a regression line for each
of them. Here we are using the <code class="docutils literal"><span class="pre">plot_posterior_predictive_glm()</span></code>
convenience function for this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">plot_posterior_predictive_glm</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                              <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior predictive regression lines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_regression_line</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;true regression line&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior predictive regression lines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-linear_18_0.png" src="../_images/notebooks_GLM-linear_18_0.png" />
</div>
</div>
<p>As you can see, our estimated regression lines are very similar to the
true regression line. But since we only have limited data we have
<em>uncertainty</em> in our estimates, here expressed by the variability of the
lines.</p>
</div>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Usability is currently a huge hurdle for wider adoption of Bayesian
statistics.</li>
<li><code class="docutils literal"><span class="pre">PyMC3</span></code> allows GLM specification with convenient syntax borrowed
from R.</li>
<li>Posterior predictive plots allow us to evaluate fit and our
uncertainty in it.</li>
</ul>
<div class="section" id="Further-reading">
<h3>Further reading<a class="headerlink" href="#Further-reading" title="Permalink to this headline">¶</a></h3>
<p>This is the first post of a small series on Bayesian GLMs I am
preparing. Next week I will describe how the Student T distribution can
be used to perform robust linear regression.</p>
<p>Then there are also other good resources on Bayesian statistics:</p>
<ul class="simple">
<li>The excellent book <a class="reference external" href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">Doing Bayesian Data Analysis by John
Kruschke</a>.</li>
<li><a class="reference external" href="http://andrewgelman.com/">Andrew Gelman’s blog</a></li>
<li><a class="reference external" href="https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1">Baeu Cronins blog post on Probabilistic
Programming</a></li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../examples.html#howto">Howto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#applied">Applied</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../examples.html#glm">GLM</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">GLM: Linear regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-robust.html">GLM: Robust Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-robust-with-outlier-detection.html">GLM: Robust Regression with Outlier Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-model-selection.html">GLM: Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-rolling-regression.html">Rolling Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-hierarchical.html">GLM: Hierarchical Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-poisson-regression.html">GLM: Poisson Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="hierarchical_partial_pooling.html">Hierarchical Partial Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-negative-binomial-regression.html">GLM: Negative Binomial Regression</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#gaussian-processes">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#mixture-models">Mixture Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#variational-inference">Variational Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/notebooks/GLM-linear.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>