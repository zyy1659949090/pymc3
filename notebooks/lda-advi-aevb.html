
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3 &#8212; PyMC3 3.2 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '3.2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Variational Inference: Bayesian Neural Networks" href="bayesian_neural_network_advi.html" />
    <link rel="prev" title="GLM: Mini-batch ADVI on hierarchical regression model" href="GLM-hierarchical-advi-minibatch.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="Automatic-autoencoding-variational-Bayes-for-latent-dirichlet-allocation-with-PyMC3">
<h1>Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3<a class="headerlink" href="#Automatic-autoencoding-variational-Bayes-for-latent-dirichlet-allocation-with-PyMC3" title="Permalink to this headline">Â¶</a></h1>
<p>For probabilistic models with latent variables, autoencoding variational
Bayes (AEVB; Kingma and Welling, 2014) is an algorithm which allows us
to perform inference efficiently for large datasets with an encoder. In
AEVB, the encoder is used to infer variational parameters of approximate
posterior on latent variables from given samples. By using tunable and
flexible encoders such as multilayer perceptrons (MLPs), AEVB
approximates complex variational posterior based on mean-field
approximation, which does not utilize analytic representations of the
true posterior. Combining AEVB with ADVI (Kucukelbir et al., 2015), we
can perform posterior inference on almost arbitrary probabilistic models
involving continuous latent variables.</p>
<p>I have implemented AEVB for ADVI with mini-batch on PyMC3. To
demonstrate flexibility of this approach, we will apply this to latent
dirichlet allocation (LDA; Blei et al., 2003) for modeling documents. In
the LDA model, each document is assumed to be generated from a
multinomial distribution, whose parameters are treated as latent
variables. By using AEVB with an MLP as an encoder, we will fit the LDA
model to the 20-newsgroups dataset.</p>
<p>In this example, extracted topics by AEVB seem to be qualitatively
comparable to those with a standard LDA implementation, i.e., online VB
implemented on scikit-learn. Unfortunately, the predictive accuracy of
unseen words is less than the standard implementation of LDA, it might
be due to the mean-field approximation. However, the combination of AEVB
and ADVI allows us to quickly apply more complex probabilistic models
than LDA to big data with the help of mini-batches. I hope this notebook
will attract readers, especially practitioners working on a variety of
machine learning tasks, to probabilistic programming and PyMC3.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">os</span>
<span class="c1"># unfortunately I was not able to run it on GPU due to overflow problems</span>
<span class="o">%</span><span class="k">env</span> THEANO_FLAGS=device=cpu,floatX=float64
<span class="kn">import</span> <span class="nn">theano</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">shared</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">tt</span>
<span class="kn">from</span> <span class="nn">theano.sandbox.rng_mrg</span> <span class="kn">import</span> <span class="n">MRG_RandomStreams</span>

<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">math</span> <span class="k">as</span> <span class="n">pmmath</span>
<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Dirichlet</span>
<span class="kn">from</span> <span class="nn">pymc3.distributions.transforms</span> <span class="kn">import</span> <span class="n">t_stick_breaking</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
env: THEANO_FLAGS=device=cpu,floatX=float64
</pre></div></div>
</div>
<div class="section" id="Dataset">
<h2>Dataset<a class="headerlink" href="#Dataset" title="Permalink to this headline">Â¶</a></h2>
<p>Here, we will use the 20-newsgroups dataset. This dataset can be
obtained by using functions of scikit-learn. The below code is partially
adopted from an example of scikit-learn
(<a class="reference external" href="http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html">http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html</a>).
We set the number of words in the vocabulary to 1000.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># The number of words in the vocaburary</span>
<span class="n">n_words</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading dataset...&quot;</span><span class="p">)</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">))</span>
<span class="n">data_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>

<span class="c1"># Use tf (raw term count) features for LDA.</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Extracting tf features for LDA...&quot;</span><span class="p">)</span>
<span class="n">tf_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">n_words</span><span class="p">,</span>
                                <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">tf</span> <span class="o">=</span> <span class="n">tf_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_samples</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">tf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;done in </span><span class="si">%0.3f</span><span class="s2">s.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loading dataset...
done in 1.793s.
Extracting tf features for LDA...
done in 3.352s.
</pre></div></div>
</div>
<p>Each document is represented by 1000-dimensional term-frequency vector.
Letâs check the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_lda-advi-aevb_5_0.png" src="../_images/notebooks_lda-advi-aevb_5_0.png" />
</div>
</div>
<p>We split the whole documents into training and test sets. The number of
tokens in the training set is 480K. Sparsity of the term-frequency
document matrix is 0.025%, which implies almost all components in the
term-frequency matrix is zero.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">n_samples_tr</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_samples_te</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">n_samples_tr</span>
<span class="n">docs_tr</span> <span class="o">=</span> <span class="n">tf</span><span class="p">[:</span><span class="n">n_samples_tr</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">docs_te</span> <span class="o">=</span> <span class="n">tf</span><span class="p">[</span><span class="n">n_samples_tr</span><span class="p">:,</span> <span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of docs for training = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of docs for test = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">docs_te</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">n_tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">docs_tr</span><span class="p">[</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of tokens in training set = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Sparsity = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">docs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of docs for training = 10000
Number of docs for test = 1314
Number of tokens in training set = 480263
Sparsity = 0.0253936
</pre></div></div>
</div>
</div>
<div class="section" id="Log-likelihood-of-documents-for-LDA">
<h2>Log-likelihood of documents for LDA<a class="headerlink" href="#Log-likelihood-of-documents-for-LDA" title="Permalink to this headline">Â¶</a></h2>
<p>For a document <span class="math">\(d\)</span> consisting of tokens <span class="math">\(w\)</span>, the
log-likelihood of the LDA model with <span class="math">\(K\)</span> topics is given as</p>
<div class="math">
\begin{eqnarray}
    \log p\left(d|\theta_{d},\beta\right) &amp; = &amp; \sum_{w\in d}\log\left[\sum_{k=1}^{K}\exp\left(\log\theta_{d,k} + \log \beta_{k,w}\right)\right]+const,
\end{eqnarray}</div><p>where <span class="math">\(\theta_{d}\)</span> is the topic distribution for document
<span class="math">\(d\)</span> and <span class="math">\(\beta\)</span> is the word distribution for the <span class="math">\(K\)</span>
topics. We define a function that returns a tensor of the log-likelihood
of documents given <span class="math">\(\theta_{d}\)</span> and <span class="math">\(\beta\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">logp_lda_doc</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the log-likelihood function for given documents.</span>

<span class="sd">    K : number of topics in the model</span>
<span class="sd">    V : number of words (size of vocabulary)</span>
<span class="sd">    D : number of documents (in a mini-batch)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    beta : tensor (K x V)</span>
<span class="sd">        Word distributions.</span>
<span class="sd">    theta : tensor (D x K)</span>
<span class="sd">        Topic distributions for documents.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">ll_docs_f</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
        <span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
        <span class="n">vfreqs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span><span class="p">]</span>
        <span class="n">ll_docs</span> <span class="o">=</span> <span class="n">vfreqs</span> <span class="o">*</span> <span class="n">pmmath</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="n">dixs</span><span class="p">])</span> <span class="o">+</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">vixs</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="c1"># Per-word log-likelihood times num of tokens in the whole dataset</span>
        <span class="k">return</span> <span class="n">tt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ll_docs</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vfreqs</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-9</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_tokens</span>

    <span class="k">return</span> <span class="n">ll_docs_f</span>
</pre></div>
</div>
</div>
<p>In the inner function, the log-likelihood is scaled for mini-batches by
the number of tokens in the dataset.</p>
</div>
<div class="section" id="LDA-model">
<h2>LDA model<a class="headerlink" href="#LDA-model" title="Permalink to this headline">Â¶</a></h2>
<p>With the log-likelihood function, we can construct the probabilistic
model for LDA. <code class="docutils literal"><span class="pre">doc_t</span></code> works as a placeholder to which documents in a
mini-batch are set.</p>
<p>For ADVI, each of random variables <span class="math">\(\theta\)</span> and <span class="math">\(\beta\)</span>,
drawn from Dirichlet distributions, is transformed into unconstrained
real coordinate space. To do this, by default, PyMC3 uses a centered
stick-breaking transformation. Since these random variables are on a
simplex, the dimension of the unconstrained coordinate space is the
original dimension minus 1. For example, the dimension of
<span class="math">\(\theta_{d}\)</span> is the number of topics (<code class="docutils literal"><span class="pre">n_topics</span></code>) in the LDA
model, thus the transformed space has dimension <code class="docutils literal"><span class="pre">(n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code>. It
shuold be noted that, in this example, we use <code class="docutils literal"><span class="pre">t_stick_breaking</span></code>,
which is a numerically stable version of <code class="docutils literal"><span class="pre">stick_breaking</span></code> used by
default. This is required to work ADVI for the LDA model.</p>
<p>The variational posterior on these transformed parameters is represented
by a spherical Gaussian distributions (meanfield approximation). Thus,
the number of variational parameters of <span class="math">\(\theta_{d}\)</span>, the latent
variable for each document, is <code class="docutils literal"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">(n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code> for means and
standard deviations.</p>
<p>In the last line of the below cell, <code class="docutils literal"><span class="pre">DensityDist</span></code> class is used to
define the log-likelihood function of the model. The second argument is
a Python function which takes observations (a document matrix in this
example) and returns the log-likelihood value. This function is given as
a return value of <code class="docutils literal"><span class="pre">logp_lda_doc(beta,</span> <span class="pre">theta)</span></code>, which has been defined
above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># we have sparse dataset. It&#39;s better to have dence batch so that all words accure there</span>
<span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># defining minibatch</span>
<span class="n">doc_t_minibatch</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Minibatch</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">minibatch_size</span><span class="p">)</span>
<span class="n">doc_t</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">toarray</span><span class="p">()[:</span><span class="n">minibatch_size</span><span class="p">])</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">((</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">))),</span>
                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">),</span>
                      <span class="c1"># do not forget scaling</span>
                      <span class="n">total_size</span><span class="o">=</span><span class="n">n_samples_tr</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">((</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_topics</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">))),</span>
                     <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">n_words</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">t_stick_breaking</span><span class="p">(</span><span class="mf">1e-9</span><span class="p">))</span>
    <span class="c1"># Note, that we devined likelihood with scaling, se here we need no additional `total_size` kwarg</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;doc&#39;</span><span class="p">,</span> <span class="n">logp_lda_doc</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">theta</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">doc_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Encoder">
<h2>Encoder<a class="headerlink" href="#Encoder" title="Permalink to this headline">Â¶</a></h2>
<p>Given a document, the encoder calculates variational parameters of the
(transformed) latent variables, more specifically, parameters of
Gaussian distributions in the unconstrained real coordinate space. The
<code class="docutils literal"><span class="pre">encode()</span></code> method is required to output variational means and stds as
a tuple, as shown in the following code. As explained above, the number
of variational parameters is <code class="docutils literal"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">(n_topics)</span> <span class="pre">-</span> <span class="pre">1</span></code>. Specifically, the
shape of <code class="docutils literal"><span class="pre">zs_mean</span></code> (or <code class="docutils literal"><span class="pre">zs_std</span></code>) in the method is
<code class="docutils literal"><span class="pre">(minibatch_size,</span> <span class="pre">n_topics</span> <span class="pre">-</span> <span class="pre">1)</span></code>. It should be noted that <code class="docutils literal"><span class="pre">zs_std</span></code>
is defined as <span class="math">\(\rho = log(exp(std) - 1)\)</span> in <code class="docutils literal"><span class="pre">ADVI</span></code> and bounded
to be positive. The inverse parametrization is
<span class="math">\(std = log(1+exp(\rho))\)</span> and considered to be numericaly stable.</p>
<p>To enhance generalization ability to unseen words, a bernoulli
corruption process is applied to the inputted documents. Unfortunately,
I have never see any significant improvement with this.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">LDAEncoder</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Encode (term-frequency) document vectors to variational means and (log-transformed) stds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_topics</span><span class="p">,</span> <span class="n">p_corruption</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">=</span> <span class="n">n_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">=</span> <span class="n">n_topics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w0</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w0&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b0</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b0&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">MRG_RandomStreams</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span> <span class="o">=</span> <span class="n">p_corruption</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span><span class="p">:</span>
            <span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">set_subtensor</span><span class="p">(</span>
                <span class="n">tt</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">xs</span><span class="p">)[</span><span class="n">dixs</span><span class="p">,</span> <span class="n">vixs</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">dixs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">p_corruption</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">xs_</span> <span class="o">=</span> <span class="n">xs</span> <span class="o">*</span> <span class="n">mask</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xs_</span> <span class="o">=</span> <span class="n">xs</span>

        <span class="n">w0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w0</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">))</span>
        <span class="n">w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">hs</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">xs_</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w0</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">)</span>
        <span class="n">zs</span> <span class="o">=</span> <span class="n">hs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
        <span class="n">zs_mean</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[:,</span> <span class="p">:(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">zs_rho</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[:,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_topics</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):]</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">zs_mean</span><span class="p">,</span> <span class="s1">&#39;rho&#39;</span><span class="p">:</span><span class="n">zs_rho</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">w0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>To feed the output of the encoder to the variational parameters of
<span class="math">\(\theta\)</span>, we set an OrderedDict of tuples as below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">encoder</span> <span class="o">=</span> <span class="n">LDAEncoder</span><span class="p">(</span><span class="n">n_words</span><span class="o">=</span><span class="n">n_words</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_topics</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">p_corruption</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">local_RVs</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="n">theta</span><span class="p">,</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">doc_t</span><span class="p">))])</span>
<span class="n">local_RVs</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[8]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>OrderedDict([(theta,
              {&#39;mu&#39;: Subtensor{::, :int64:}.0,
               &#39;rho&#39;: Subtensor{::, int64::}.0})])
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">theta</span></code> is the random variable defined in the model creation and is a
key of an entry of the <code class="docutils literal"><span class="pre">OrderedDict</span></code>. The value
<code class="docutils literal"><span class="pre">(encoder.encode(doc_t),</span> <span class="pre">n_samples_tr</span> <span class="pre">/</span> <span class="pre">minibatch_size)</span></code> is a tuple of
a theano expression and a scalar. The theano expression
<code class="docutils literal"><span class="pre">encoder.encode(doc_t)</span></code> is the output of the encoder given inputs
(documents). The scalar <code class="docutils literal"><span class="pre">n_samples_tr</span> <span class="pre">/</span> <span class="pre">minibatch_size</span></code> specifies the
scaling factor for mini-batches.</p>
<p>ADVI optimizes the parameters of the encoder. They are passed to the
function for ADVI.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">encoder_params</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
<span class="n">encoder_params</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[9]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[w0, b0, w1, b1]
</pre></div>
</div>
</div>
</div>
<div class="section" id="AEVB-with-ADVI">
<h2>AEVB with ADVI<a class="headerlink" href="#AEVB-with-ADVI" title="Permalink to this headline">Â¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="err">Î·</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="err">Î·</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">reduce_rate</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">s</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="err">Î·</span><span class="o">/</span><span class="p">((</span><span class="n">i</span><span class="o">/</span><span class="n">minibatch_size</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**.</span><span class="mi">7</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">MeanField</span><span class="p">(</span><span class="n">local_rv</span><span class="o">=</span><span class="n">local_RVs</span><span class="p">)</span>
    <span class="n">approx</span><span class="o">.</span><span class="n">scale_cost_to_minibatch</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">inference</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">KLqp</span><span class="p">(</span><span class="n">approx</span><span class="p">)</span>
<span class="n">inference</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">reduce_rate</span><span class="p">],</span> <span class="n">obj_optimizer</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">s</span><span class="p">),</span>
              <span class="n">more_obj_params</span><span class="o">=</span><span class="n">encoder_params</span><span class="p">,</span> <span class="n">total_grad_norm_constraint</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
              <span class="n">more_replacements</span><span class="o">=</span><span class="p">{</span><span class="n">doc_t</span><span class="p">:</span><span class="n">doc_t_minibatch</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Average Loss = 2.985e+06: 100%|ââââââââââ| 10000/10000 [02:26&lt;00:00, 67.52it/s]
Finished [100%]: Average Loss = 2.9882e+06
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[10]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>&lt;pymc3.variational.approximations.MeanField at 0x10cebb278&gt;
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="n">approx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Approximation{MeanFieldGroup[9990] &amp; MeanFieldGroup[None, 9]}
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">approx</span><span class="o">.</span><span class="n">hist</span><span class="p">[</span><span class="mi">10</span><span class="p">:]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_lda-advi-aevb_22_0.png" src="../_images/notebooks_lda-advi-aevb_22_0.png" />
</div>
</div>
</div>
<div class="section" id="Extraction-of-characteristic-words-of-topics-based-on-posterior-samples">
<h2>Extraction of characteristic words of topics based on posterior samples<a class="headerlink" href="#Extraction-of-characteristic-words-of-topics-based-on-posterior-samples" title="Permalink to this headline">Â¶</a></h2>
<p>By using estimated variational parameters, we can draw samples from the
variational posterior. To do this, we use function <code class="docutils literal"><span class="pre">sample_vp()</span></code>. Here
we use this function to obtain posterior mean of the word-topic
distribution <span class="math">\(\beta\)</span> and show top-10 words frequently appeared in
the 10 topics.</p>
<p>To apply the above function for the LDA model, we redefine the
probabilistic model because the number of documents to be tested
changes. Since variational parameters have already been obtained, we can
reuse them for sampling from the approximate posterior distribution.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">print_top_words</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_words</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">beta</span><span class="p">)):</span>
        <span class="k">print</span><span class="p">((</span><span class="s2">&quot;Topic #</span><span class="si">%d</span><span class="s2">: &quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">feature_names</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="o">-</span><span class="n">n_top_words</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>


<span class="n">doc_t</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">docs_tr</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_approx</span><span class="p">(</span><span class="n">approx</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">beta_pymc3</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">print_top_words</span><span class="p">(</span><span class="n">beta_pymc3</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Topic #0: people think god don just said know say like time
Topic #1: use file windows drive using program key does scsi like
Topic #2: just like don good year time think car team game
Topic #3: ax max g9v b8f 75u a86 bhj pl 1t giz
Topic #4: 00 10 25 11 15 17 20 55 12 16
Topic #5: edu com mail list send email cs address like don
Topic #6: space new information nasa use data research 1993 program university
Topic #7: know like thanks just don does good new mail help
Topic #8: know like does just thanks don good new mail use
Topic #9: know like does just don thanks good new think mail
</pre></div></div>
</div>
<p>We compare these topics to those obtained by a standard LDA
implementation on scikit-learn, which is based on an online stochastic
variational inference (Hoffman et al., 2013). We can see that estimated
words in the topics are qualitatively similar.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_topics</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                <span class="n">learning_method</span><span class="o">=</span><span class="s1">&#39;online&#39;</span><span class="p">,</span> <span class="n">learning_offset</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">%</span><span class="k">time</span> lda.fit(docs_tr)
<span class="n">beta_sklearn</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">components_</span> <span class="o">/</span> <span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">print_top_words</span><span class="p">(</span><span class="n">beta_sklearn</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 15.6 s, sys: 57.9 ms, total: 15.7 s
Wall time: 15.8 s
Topic #0: people gun armenian war armenians turkish states said state 000
Topic #1: government people law mr president use don think right public
Topic #2: space science nasa program data research center output earth launch
Topic #3: key car chip used keys bit bike clipper use number
Topic #4: edu file com mail available ftp image files information list
Topic #5: god people does jesus think believe don say just know
Topic #6: windows drive use thanks does card know problem like db
Topic #7: ax max g9v pl b8f a86 cx 34u 145 1t
Topic #8: just don like know think good time ve people year
Topic #9: 00 10 25 15 20 12 11 16 14 17
</pre></div></div>
</div>
</div>
<div class="section" id="Predictive-distribution">
<h2>Predictive distribution<a class="headerlink" href="#Predictive-distribution" title="Permalink to this headline">Â¶</a></h2>
<p>In some papers (e.g., Hoffman et al. 2013), the predictive distribution
of held-out words was proposed as a quantitative measure for goodness of
the model fitness. The log-likelihood function for tokens of the
held-out word can be calculated with posterior means of <span class="math">\(\theta\)</span>
and <span class="math">\(\beta\)</span>. The validity of this is explained in (Hoffman et al.
2013).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">calc_pp</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">wix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    ws: ndarray (N,)</span>
<span class="sd">        Number of times the held-out word appeared in N documents.</span>
<span class="sd">    thetas: ndarray, shape=(N, K)</span>
<span class="sd">        Topic distributions for N documents.</span>
<span class="sd">    beta: ndarray, shape=(K, V)</span>
<span class="sd">        Word distributions for K topics.</span>
<span class="sd">    wix: int</span>
<span class="sd">        Index of the held-out word</span>

<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    Log probability of held-out words.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ws</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">thetas</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">eval_lda</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">docs_te</span><span class="p">,</span> <span class="n">wixs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate LDA model by log predictive probability.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    transform: Python function</span>
<span class="sd">        Transform document vectors to posterior mean of topic proportions.</span>
<span class="sd">    wixs: iterable of int</span>
<span class="sd">        Word indices to be held-out.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lpss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">docs_</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">docs_te</span><span class="p">)</span>
    <span class="n">thetass</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">wss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_words</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">wix</span> <span class="ow">in</span> <span class="n">wixs</span><span class="p">:</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">docs_te</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">():</span>
            <span class="c1"># Hold-out</span>
            <span class="n">docs_</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Topic distributions</span>
            <span class="n">thetas</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">docs_</span><span class="p">)</span>

            <span class="c1"># Predictive log probability</span>
            <span class="n">lpss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_pp</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">wix</span><span class="p">))</span>

            <span class="n">docs_</span><span class="p">[:,</span> <span class="n">wix</span><span class="p">]</span> <span class="o">=</span> <span class="n">ws</span>
            <span class="n">thetass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>
            <span class="n">wss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ws</span><span class="p">)</span>
            <span class="n">total_words</span> <span class="o">+=</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">thetass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
            <span class="n">wss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>

    <span class="c1"># Log-probability</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">lpss</span><span class="p">))</span> <span class="o">/</span> <span class="n">total_words</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lp&#39;</span><span class="p">:</span> <span class="n">lp</span><span class="p">,</span>
        <span class="s1">&#39;thetass&#39;</span><span class="p">:</span> <span class="n">thetass</span><span class="p">,</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;wss&#39;</span><span class="p">:</span> <span class="n">wss</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">transform()</span></code> function is defined with <code class="docutils literal"><span class="pre">sample_vp()</span></code> function. This
function is an argument to the function for calculating log predictive
probabilities.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int64&#39;</span><span class="p">)</span>
<span class="n">sample_vi_theta</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="p">[</span><span class="n">inp</span><span class="p">],</span>
    <span class="n">approx</span><span class="o">.</span><span class="n">sample_node</span><span class="p">(</span><span class="n">approx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span>  <span class="n">more_replacements</span><span class="o">=</span><span class="p">{</span><span class="n">doc_t</span><span class="p">:</span> <span class="n">inp</span><span class="p">})</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">transform_pymc3</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sample_vi_theta</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">time</span> result_pymc3 = eval_lda(transform_pymc3, beta_pymc3, docs_te.toarray(), np.arange(100))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Predictive log prob (pm3) = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result_pymc3</span><span class="p">[</span><span class="s1">&#39;lp&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 34.8 s, sys: 3.28 s, total: 38.1 s
Wall time: 37.7 s
Predictive log prob (pm3) = -6.2451802630496385
</pre></div></div>
</div>
<p>We compare the result with the scikit-learn LDA implemented The log
predictive probability is comparable (-6.24) with AEVB-ADVI, and it
shows good set of words in the estimated topics.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">transform_sklearn</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">thetas</span> <span class="o">/</span> <span class="n">thetas</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="o">%</span><span class="k">time</span> result_sklearn = eval_lda(transform_sklearn, beta_sklearn, docs_te.toarray(), np.arange(100))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Predictive log prob (sklearn) = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result_sklearn</span><span class="p">[</span><span class="s1">&#39;lp&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 37.1 s, sys: 205 ms, total: 37.3 s
Wall time: 37.7 s
Predictive log prob (sklearn) = -6.014771065227896
</pre></div></div>
</div>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">Â¶</a></h2>
<p>We have seen that PyMC3 allows us to estimate random variables of LDA, a
probabilistic model with latent variables, based on automatic
variational inference. Variational parameters of the local latent
variables in the probabilistic model are encoded from observations. The
parameters of the encoding model, MLP in this example, are optimized
with variational parameters of the global latent variables. Once the
probabilistic and the encoding models are defined, parameter
optimization is done just by invoking an inference (<code class="docutils literal"><span class="pre">ADVI()</span></code>) without
need to derive complex update equations.</p>
<p>This notebook shows that even mean field approximation can perform as
well as sklearn implementation, which is based on the conjugate priors
and thus not relying on the mean field approximation.</p>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes.
stat, 1050, 1.</li>
<li>Kucukelbir, A., Ranganath, R., Gelman, A., &amp; Blei, D. (2015).
Automatic variational inference in Stan. In Advances in neural
information processing systems (pp. 568-576).</li>
<li>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet
allocation. Journal of machine Learning research, 3(Jan), 993-1022.</li>
<li>Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. W. (2013).
Stochastic variational inference. Journal of Machine Learning
Research, 14(1), 1303-1347.</li>
<li>Rezende, D. J., &amp; Mohamed, S. (2015). Variational inference with
normalizing flows. arXiv preprint arXiv:1505.05770.</li>
<li>Salimans, T., Kingma, D. P., &amp; Welling, M. (2015). Markov chain Monte
Carlo and variational inference: Bridging the gap. In International
Conference on Machine Learning (pp. 1218-1226).</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../examples.html#howto">Howto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#applied">Applied</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#glm">GLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#gaussian-processes">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#mixture-models">Mixture Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../examples.html#variational-inference">Variational Inference</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="GLM-hierarchical-advi-minibatch.html">GLM: Mini-batch ADVI on hierarchical regression model</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesian_neural_network_advi.html">Variational Inference: Bayesian Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolutional_vae_keras_advi.html">Convolutional variational autoencoder with PyMC3 and Keras</a></li>
<li class="toctree-l3"><a class="reference internal" href="empirical-approx-overview.html">Empirical Approximation overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="normalizing_flows_overview.html">Normalizing Flows Overview</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/notebooks/lda-advi-aevb.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>